{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import nltk\n",
        "import re\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "cp0QyoxYLnxV",
        "outputId": "336d1613-b9ca-4bbf-ff07-b15158e2ed9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(a_set, cats):\n",
        "    dataset = fetch_20newsgroups(subset=a_set, categories=cats,\n",
        "                                remove=('headers', 'footers', 'quotes'),\n",
        "                                shuffle=True)\n",
        "    return dataset\n",
        "\n",
        "categories = [\"comp.windows.x\", \"misc.forsale\", \"rec.autos\", \"rec.motorcycles\",\n",
        "            \"rec.sport.baseball\", \"rec.sport.hockey\", \"sci.crypt\", \"sci.med\",\n",
        "            \"sci.space\", \"talk.politics.mideast\"]"
      ],
      "metadata": {
        "id": "xDOOUizbLn2U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newsgroups_all = load_dataset('all', categories)\n",
        "print(f\"Loaded {len(newsgroups_all.data)} docs.\")"
      ],
      "metadata": {
        "id": "58lu6gbwLnzz",
        "outputId": "94dcf2ef-102c-4f0a-f7fb-d422b68aefe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 9850 docs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    # Usuń znaki specjalne i liczby używając wyrażenia regularnego\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "     # Podziel tekst na tokeny, usuwając stop words i stosując stemming\n",
        "    tokens = [stemmer.stem(word) for word in text.lower().split() if word not in stop_words]\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "fPFhSFbpLnu8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_docs = [preprocess(doc) for doc in newsgroups_all.data]"
      ],
      "metadata": {
        "id": "IX8eksTeMcj3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF"
      ],
      "metadata": {
        "id": "0RKI9Pg70IXz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Wybierz dwa z LSA, LDA i NMF, zaimplementuj te rozwiązania i porównaj wyniki\n",
        "# wyświetl 10 tematów, porównaj je ze sobą\n",
        "# wyświetl artykuły (albo jego część np. 100 słów) dla jednego z tematów i zobacz co na jego temat mają do powiedzenia modele\n",
        "\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=1000)\n",
        "tfidf = vectorizer.fit_transform(preprocessed_docs)\n",
        "\n",
        "# LDA\n",
        "lda = LatentDirichletAllocation(n_components=10, random_state=42)\n",
        "lda_topics = lda.fit_transform(tfidf)\n",
        "\n",
        "# NMF\n",
        "nmf = NMF(n_components=10, random_state=42)\n",
        "nmf_topics = nmf.fit_transform(tfidf)\n",
        "\n",
        "# Function to display topics\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(f\"Topic {topic_idx + 1}:\")\n",
        "        print(\" \".join([feature_names[i]\n",
        "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "# Display topics for LDA and NMF\n",
        "no_top_words = 10\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "print(\"LDA Topics:\")\n",
        "display_topics(lda, feature_names, no_top_words)\n",
        "print(\"\\nNMF Topics:\")\n",
        "display_topics(nmf, feature_names, no_top_words)\n"
      ],
      "metadata": {
        "id": "OTv1dDWdkyQT",
        "outputId": "513c3c1e-b119-48fc-9d5c-01c264af01db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LDA Topics:\n",
            "Topic 1:\n",
            "dog motorcycl univers new delet april pictur lost th california\n",
            "Topic 2:\n",
            "game team hockey basebal fan play year player go think\n",
            "Topic 3:\n",
            "game player play year team hit pitch win score run\n",
            "Topic 4:\n",
            "key use email pleas sale offer encrypt system chip includ\n",
            "Topic 5:\n",
            "car bike would like one get use engin ride dont\n",
            "Topic 6:\n",
            "israel armenian peopl jew arab space isra us would muslim\n",
            "Topic 7:\n",
            "mr oh quot articl answer know want instead someth your\n",
            "Topic 8:\n",
            "window use server file program widget thank run display motif\n",
            "Topic 9:\n",
            "david van pit bos la det trust nsa think say\n",
            "Topic 10:\n",
            "one would dont know peopl like get effect think use\n",
            "\n",
            "NMF Topics:\n",
            "Topic 1:\n",
            "like dont would get one think go know im thing\n",
            "Topic 2:\n",
            "team year player play season last win leagu hockey hit\n",
            "Topic 3:\n",
            "thank pleas email anyon know list mail post send address\n",
            "Topic 4:\n",
            "key chip encrypt clipper use secur govern phone escrow system\n",
            "Topic 5:\n",
            "armenian turkish muslim peopl genocid armenia turk turkey govern russian\n",
            "Topic 6:\n",
            "sale offer new drive ship includ price condit sell ask\n",
            "Topic 7:\n",
            "use window run program file server applic display widget set\n",
            "Topic 8:\n",
            "game basebal score win show hockey watch night playoff play\n",
            "Topic 9:\n",
            "car engin mile dealer new drive oil buy speed bike\n",
            "Topic 10:\n",
            "israel arab isra jew jewish palestinian state right kill peopl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LAACUwu20LY5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}