{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnXzcZvdc-r6"
      },
      "source": [
        "In this notebook we will demostrate how to perform tokenization,stemming,lemmatization and pos_tagging using libraries like [spacy](https://spacy.io/) and [nltk](https://www.nltk.org/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIlzYU8p2aTw",
        "outputId": "35d648f5-8471-4977-c3aa-85a6f5f6c283"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "fARt_uKHAf4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf94bfa-c8a3-4164-9a86-f2e6f950bf0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizacja"
      ],
      "metadata": {
        "id": "oMcAUqyiAiqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Przykładowy tekst\n",
        "txt = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
        "\n",
        "\n",
        "# Tokenizacja zdań i słów za pomocą spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(txt)\n",
        "\n",
        "sentences_spacy = [sent.text for sent in doc.sents]\n",
        "print(\"Tokenizacja zdań (spaCy):\", sentences_spacy)\n",
        "\n",
        "words_spacy = [token.text for token in doc]\n",
        "print(\"Tokenizacja słów (spaCy):\", words_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omtUV9Zv8eAM",
        "outputId": "bd112993-f307-4276-fda0-f711523141b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizacja zdań (spaCy): ['Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!.', 'It should be done by the ending of this month.', 'But will it?', 'This notebook has been run 4 times !!']\n",
            "Tokenizacja słów (spaCy): ['Need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'and', 'it', 'should', 'be', 'done', 'soon', '!', '!', '.', 'It', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', '.', 'But', 'will', 'it', '?', 'This', 'notebook', 'has', 'been', 'run', '4', 'times', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Tokenizacja zdań (NLTK):\")\n",
        "sentences_nltk = sent_tokenize(txt)\n",
        "print(sentences_nltk)\n",
        "\n",
        "print(\"Tokenizacja słów (NLTK):\")\n",
        "words_nltk = word_tokenize(txt)\n",
        "print(words_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYShFW2XXaje",
        "outputId": "799ac9de-2474-4db3-9365-dac5322d02a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizacja zdań (NLTK):\n",
            "['Need to finalize the demo corpus which will be used for this notebook and it should be done soon !', '!.', 'It should be done by the ending of this month.', 'But will it?', 'This notebook has been run 4 times !', '!']\n",
            "Tokenizacja słów (NLTK):\n",
            "['Need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'and', 'it', 'should', 'be', 'done', 'soon', '!', '!', '.', 'It', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', '.', 'But', 'will', 'it', '?', 'This', 'notebook', 'has', 'been', 'run', '4', 'times', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tagowanie POS"
      ],
      "metadata": {
        "id": "VQM7yPLdGM5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tagowanie POS\n",
        "pos_tags_spacy = [(token.text, token.pos_) for token in doc]\n",
        "print(\"Tagowanie POS:\", pos_tags_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpgdPjg5GJPP",
        "outputId": "5c6e1ba1-4eb7-4736-d2bf-0a83fbdecdb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tagowanie POS: [('Need', 'VERB'), ('to', 'PART'), ('finalize', 'VERB'), ('the', 'DET'), ('demo', 'NOUN'), ('corpus', 'X'), ('which', 'PRON'), ('will', 'AUX'), ('be', 'AUX'), ('used', 'VERB'), ('for', 'ADP'), ('this', 'DET'), ('notebook', 'NOUN'), ('and', 'CCONJ'), ('it', 'PRON'), ('should', 'AUX'), ('be', 'AUX'), ('done', 'VERB'), ('soon', 'ADV'), ('!', 'PUNCT'), ('!', 'PUNCT'), ('.', 'PUNCT'), ('It', 'PRON'), ('should', 'AUX'), ('be', 'AUX'), ('done', 'VERB'), ('by', 'ADP'), ('the', 'DET'), ('ending', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('month', 'NOUN'), ('.', 'PUNCT'), ('But', 'CCONJ'), ('will', 'AUX'), ('it', 'PRON'), ('?', 'PUNCT'), ('This', 'DET'), ('notebook', 'NOUN'), ('has', 'AUX'), ('been', 'AUX'), ('run', 'VERB'), ('4', 'NUM'), ('times', 'NOUN'), ('!', 'PUNCT'), ('!', 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags_nltk = pos_tag(words_nltk)\n",
        "print(\"POS tags:\", pos_tags_nltk)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-drblMSWk-A",
        "outputId": "5bf3b168-6288-49b3-99eb-764602779850"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS tags: [('Need', 'NN'), ('to', 'TO'), ('finalize', 'VB'), ('the', 'DT'), ('demo', 'NN'), ('corpus', 'NN'), ('which', 'WDT'), ('will', 'MD'), ('be', 'VB'), ('used', 'VBN'), ('for', 'IN'), ('this', 'DT'), ('notebook', 'NN'), ('and', 'CC'), ('it', 'PRP'), ('should', 'MD'), ('be', 'VB'), ('done', 'VBN'), ('soon', 'RB'), ('!', '.'), ('!', '.'), ('.', '.'), ('It', 'PRP'), ('should', 'MD'), ('be', 'VB'), ('done', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('ending', 'VBG'), ('of', 'IN'), ('this', 'DT'), ('month', 'NN'), ('.', '.'), ('But', 'CC'), ('will', 'MD'), ('it', 'PRP'), ('?', '.'), ('This', 'DT'), ('notebook', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('run', 'VBN'), ('4', 'CD'), ('times', 'NNS'), ('!', '.'), ('!', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "PHNxZFkcLtNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tematyzacja (Stemming)"
      ],
      "metadata": {
        "id": "IjcLyiS6As2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming za pomocą NLTK\n",
        "# sprawdź działanie innych stemmerów\n",
        "stemmer = PorterStemmer()\n",
        "stems_nltk = [stemmer.stem(word) for word in words_spacy]\n",
        "print(\"Steaming (NLTK):\", stems_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aesHVlaSAv4f",
        "outputId": "013582a4-d3e0-4cba-f481-b5498f3698d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steaming (NLTK): ['need', 'to', 'final', 'the', 'demo', 'corpu', 'which', 'will', 'be', 'use', 'for', 'thi', 'notebook', 'and', 'it', 'should', 'be', 'done', 'soon', '!', '!', '.', 'it', 'should', 'be', 'done', 'by', 'the', 'end', 'of', 'thi', 'month', '.', 'but', 'will', 'it', '?', 'thi', 'notebook', 'ha', 'been', 'run', '4', 'time', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lematyzacja"
      ],
      "metadata": {
        "id": "kxLllsF9A0Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lematyzacja za pomocą spaCy\n",
        "lemmas_spacy = [token.lemma_ for token in doc]\n",
        "print(\"Lematyzacja (spaCy):\", lemmas_spacy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7kCaB_7BQdZ",
        "outputId": "9ab956a7-67b5-4b73-efab-0616ae27ef3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lematyzacja (spaCy): ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'use', 'for', 'this', 'notebook', 'and', 'it', 'should', 'be', 'do', 'soon', '!', '!', '.', 'it', 'should', 'be', 'do', 'by', 'the', 'ending', 'of', 'this', 'month', '.', 'but', 'will', 'it', '?', 'this', 'notebook', 'have', 'be', 'run', '4', 'time', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"The children are playing outside.\")\n",
        "print([token.lemma_ for token in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ktd1xWAVELEI",
        "outputId": "f63d719f-15d1-40a4-d80b-6429c7b15dd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'child', 'be', 'play', 'outside', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usuwanie Słów Stopu"
      ],
      "metadata": {
        "id": "VyS70nwLEp0e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
      ],
      "metadata": {
        "id": "8_bYm8pg8hmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ENGLISH_STOP_WORDS)"
      ],
      "metadata": {
        "id": "Zr3nKIQY845e",
        "outputId": "728eaba7-cf93-4d42-c312-709ed82210b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "318"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "words = [\"this\", \"is\", \"a\", \"great\", \"movie\"]\n",
        "filtered_words = [w for w in words if not w in stop_words]\n",
        "print(filtered_words)  # Wynik: ['great', 'movie']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caqcgvEIEtrV",
        "outputId": "0f3a7b0d-d8c3-4e8c-ee26-71c3f3026446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['great', 'movie']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usuwanie Cyfr, Interpunkcji, Zmiana na małe litery"
      ],
      "metadata": {
        "id": "F6dcCwhCFL-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHh_33IopPTf",
        "outputId": "ca260b49-208e-4ab9-ceb6-05c920defe5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run 4 times !!\n"
          ]
        }
      ],
      "source": [
        "#lower case the txt\n",
        "txt = txt.lower()\n",
        "print(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yaGf8RiqgBM",
        "outputId": "8a29548d-358a-4bcb-a16d-83b60b698100"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!\n"
          ]
        }
      ],
      "source": [
        "#removing digits in the txt\n",
        "import re\n",
        "txt = re.sub(r'\\d+','', txt)\n",
        "print(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5Q--GItqzfu",
        "outputId": "e065732b-e280-455a-8301-eb5abeefcc20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "need to finalize the demo corpus which will be used for this notebook and it should be done soon  it should be done by the ending of this month but will it this notebook has been run  times \n"
          ]
        }
      ],
      "source": [
        "#removing punctuations in txt\n",
        "import string\n",
        "txt = txt.translate(str.maketrans('', '', string.punctuation))\n",
        "print(txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency"
      ],
      "metadata": {
        "id": "lxZ8-tFPlSSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fd_nltk = FreqDist(words_nltk)\n",
        "fd_nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTlOj0u-lfKX",
        "outputId": "09e7d00c-2c67-4d35-967e-48bc929ec0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'!': 4, 'be': 3, 'the': 2, 'will': 2, 'this': 2, 'notebook': 2, 'it': 2, 'should': 2, 'done': 2, '.': 2, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "fd = Counter(words_spacy)\n",
        "fd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpIjknELls5E",
        "outputId": "d3d72ae0-23c1-4fab-d9f9-5242b65a0611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Need': 1,\n",
              "         'to': 1,\n",
              "         'finalize': 1,\n",
              "         'the': 2,\n",
              "         'demo': 1,\n",
              "         'corpus': 1,\n",
              "         'which': 1,\n",
              "         'will': 2,\n",
              "         'be': 3,\n",
              "         'used': 1,\n",
              "         'for': 1,\n",
              "         'this': 2,\n",
              "         'notebook': 2,\n",
              "         'and': 1,\n",
              "         'it': 2,\n",
              "         'should': 2,\n",
              "         'done': 2,\n",
              "         'soon': 1,\n",
              "         '!': 4,\n",
              "         '.': 2,\n",
              "         'It': 1,\n",
              "         'by': 1,\n",
              "         'ending': 1,\n",
              "         'of': 1,\n",
              "         'month': 1,\n",
              "         'But': 1,\n",
              "         '?': 1,\n",
              "         'This': 1,\n",
              "         'has': 1,\n",
              "         'been': 1,\n",
              "         'run': 1,\n",
              "         '4': 1,\n",
              "         'times': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF i podobieńśtwo cosinusowe"
      ],
      "metadata": {
        "id": "wA7YFtyd8-Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tfidf = vectorizer.fit_transform(words_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8oYPNFC89r5",
        "outputId": "43a421b6-5a83-40ab-f493-12151ff0fca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
              "\twith 38 stored elements and shape (46, 27)>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSłownictwo:\")\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xuSQF079Uiz",
        "outputId": "a32b3e88-6756-4dc8-81c6-7d69b3b4ce47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Słownictwo:\n",
            "['and' 'be' 'been' 'but' 'by' 'corpus' 'demo' 'done' 'ending' 'finalize'\n",
            " 'for' 'has' 'it' 'month' 'need' 'notebook' 'of' 'run' 'should' 'soon'\n",
            " 'the' 'this' 'times' 'to' 'used' 'which' 'will']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dense_matrix = tfidf.toarray()\n",
        "print(\"\\nMacierz TF-IDF:\")\n",
        "print(dense_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQRiuNLC9Wtb",
        "outputId": "d2600bc3-ac46-4cbb-e6b6-b9625bb82055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Macierz TF-IDF:\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"primo sort corpus run\"\n",
        "query_vector = vectorizer.transform([query])"
      ],
      "metadata": {
        "id": "pBxsybmX9nrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarities = cosine_similarity(tfidf, query_vector).flatten()\n",
        "similarities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-_3I7vw9dct",
        "outputId": "fc2616ac-b3dd-437d-a4c0-5d07b6f62e7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.70710678, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.70710678, 0.        , 0.        , 0.        ,\n",
              "       0.        ])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}